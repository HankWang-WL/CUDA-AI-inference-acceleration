# üöÄ CUDA AI Inference Acceleration: Batched MatMul (PyTorch vs. Custom CUDA Kernel)

This project demonstrates how to implement and benchmark high-efficiency batched matrix multiplication on GPU using both **PyTorch (official GPU API)** and a **custom CUDA batched matmul kernel** (via Cupy dynamic loading). It provides a clear, practical baseline for AI/GPU optimization and infrastructure interviews or technical portfolio.

---

## üåü Key Features

* **PyTorch Baseline**: Uses PyTorch's batched GPU matmul as an industry-standard reference.
* **Custom CUDA Kernel**: Implements a hand-crafted CUDA batched matmul kernel, called from Python via Cupy for direct performance comparison.
* **Unified Benchmark**: Benchmark script runs both PyTorch and custom CUDA kernel for direct latency comparison.
* **Extensible Structure**: Easily extendable for ONNX, TensorRT, OpenMP, profiling, or advanced topics.

---

## üìÅ Directory Structure

```
cuda-ai-inference-acceleration/
‚îú‚îÄ‚îÄ cuda_kernel/
‚îÇ   ‚îú‚îÄ‚îÄ batched_matmul.cu         # CUDA kernel
‚îÇ   ‚îú‚îÄ‚îÄ batched_matmul.py         # Python interface for CUDA kernel
‚îÇ   ‚îú‚îÄ‚îÄ build.sh                  # (optional) build script
‚îú‚îÄ‚îÄ pytorch_baseline/
‚îÇ   ‚îî‚îÄ‚îÄ run_pytorch.py            # PyTorch baseline
‚îú‚îÄ‚îÄ benchmark/
‚îÇ   ‚îî‚îÄ‚îÄ run_all.py                # Unified benchmark
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt
```

---

## üßë‚Äçüíª Main Components

### 1. **PyTorch Baseline**

`pytorch_baseline/run_pytorch.py`

> Batched matmul on GPU using PyTorch, serving as the industry baseline.

### 2. **Custom CUDA Kernel (Cupy)**

`cuda_kernel/batched_matmul.cu`

> Pure CUDA batched matmul kernel, batch dimension parallelization.

`cuda_kernel/batched_matmul.py`

> Loads and calls CUDA kernel via cupy.RawModule, measures execution time in Python.

### 3. **Benchmark Script**

`benchmark/run_all.py`

> Runs both PyTorch and custom CUDA kernel, prints latency results for comparison.

---

## üö¶ How to Run

### 1Ô∏è‚É£ Install dependencies

```bash
pip install -r requirements.txt
# (Requires NVIDIA GPU drivers & CUDA toolkit installed)
```

### 2Ô∏è‚É£ Run benchmarks

```bash
python benchmark/run_all.py
```

Runs both the PyTorch and custom CUDA kernel batched matmul, outputs average per-batch latency (ms).

---

## üìå Technical Highlights

* **PyTorch/Deep Learning Engineering**: Shows proficiency with PyTorch GPU tensor operations.
* **CUDA Kernel Programming**: Demonstrates low-level kernel design for batched matmul, including memory layout, grid/block/thread setup.
* **Cupy Integration**: Seamlessly connects Python to CUDA with dynamic compilation, no pybind11 or manual C++ compilation needed.
* **Performance Profiling**: Unified script for direct comparison, a must-have for AI/infra teams.

---

## üî• Extension Ideas

* Add kernel profiling (nvprof, nsight, torch profiler) to visualize execution time and memory usage.
* Extend to support half precision (FP16), larger batch/matrix sizes.
* Add OpenMP-based CPU version for full CPU vs. GPU comparison.
* Integrate ONNX/TensorRT in the same benchmarking framework for end-to-end inference acceleration comparison.

---

## üì£ Why This Project?

This project was built completely from scratch to demonstrate the full workflow required for AI engineering/infra teams: from using high-level GPU frameworks (PyTorch) to hand-optimized CUDA kernels, plus real benchmark and profiling analysis. It proves capability in both software stack integration and low-level optimization, perfect for interviews at NVIDIA, Qualcomm, AWS, or similar AI/GPU teams.

---

## üë§ Author

Wang Chen Han
[hank851107@gmail.com](mailto:hank851107@gmail.com)
[GitHub: HankWang-WL](https://github.com/HankWang-WL)

---

## License

MIT License
